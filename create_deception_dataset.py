import json
import os
from collections import defaultdict
from datetime import datetime
from datasets import load_dataset

def load_taxonomy_data(file_path):
    """final_deceptive_taxonomy íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  sub_strategiesë¥¼ ì¶”ì¶œ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    taxonomy_mapping = {}
    for item in data:
        strategy_name = item['strategy_name']
        for sub_strategy in item['sub_strategies']:
            taxonomy_mapping[sub_strategy] = strategy_name
    
    return taxonomy_mapping

def load_open_coding_data(directory):
    """open_coding v1~v10 íŒŒì¼ë“¤ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œ"""
    open_coding_data = defaultdict(list)
    
    # v1~v10 íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ë¡œë“œ
    for filename in os.listdir(directory):
        if filename.startswith('open_coding_v') and filename.endswith('.json'):
            version = filename.split('_')[2]  # v1, v2, ... v10 ì¶”ì¶œ
            file_path = os.path.join(directory, filename)
            
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            for item in data:
                strategy_name = item['strategy_name']
                item['version'] = version
                open_coding_data[strategy_name].append(item)
    
    return open_coding_data

def load_huggingface_dataset():
    """í—ˆê¹…í˜ì´ìŠ¤ Anthropic/persuasion ë°ì´í„°ì…‹ì„ ë¡œë“œ"""
    try:
        print("ğŸ”„ í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë”© ì¤‘...")
        dataset = load_dataset("Anthropic/persuasion", split="train")
        print(f"ğŸ“Š ì´ {len(dataset)} ê°œì˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ")
        
        # ë°ì´í„°ë¥¼ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë³€í™˜
        hf_data = []
        for item in dataset:
            hf_data.append({
                'worker_id': item.get('worker_id', ''),
                'claim': item.get('claim', ''),
                'argument': item.get('argument', ''),
                'source': item.get('source', ''),
                'rating_initial': item.get('rating_initial', ''),
                'rating_final': item.get('rating_final', ''),
                'persuasiveness_metric': item.get('persuasiveness_metric', ''),
                'prompt_type': item.get('prompt_type', ''),
                'original_index': len(hf_data)  # ì›ë³¸ ì¸ë±ìŠ¤ ì €ì¥
            })
        
        return hf_data
        
    except Exception as e:
        print(f"âŒ í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë”© ì‹¤íŒ¨: {e}")
        return []

def create_matched_dataset(taxonomy_mapping, open_coding_data):
    """taxonomyì˜ sub_strategiesì™€ open_codingì˜ strategy_nameì„ ë§¤ì¹­"""
    matched_dataset = []
    
    for sub_strategy, taxonomy_strategy in taxonomy_mapping.items():
        if sub_strategy in open_coding_data:
            # ê° entryì— taxonomy_strategy ì •ë³´ ì¶”ê°€í•˜ì—¬ í”Œë«í•œ ë°°ì—´ë¡œ ë§Œë“¦
            for entry in open_coding_data[sub_strategy]:
                # ê¸°ì¡´ entryë¥¼ ë³µì‚¬í•˜ê³  taxonomy_strategy ì •ë³´ ì¶”ê°€
                enhanced_entry = entry.copy()
                enhanced_entry['taxonomy_strategy'] = taxonomy_strategy
                matched_dataset.append(enhanced_entry)
    
    return matched_dataset

def connect_worker_ids(matched_dataset, hf_data):
    """ë§¤ì¹­ëœ ë°ì´í„°ì…‹ê³¼ í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ì„ worker_idë¡œ ì—°ê²°"""
    print("ğŸ”— Worker ID ì—°ê²° ìˆ˜í–‰ ì¤‘...")
    
    # í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ë¥¼ worker_idë¡œ ì¸ë±ì‹±
    hf_by_worker_id = defaultdict(list)
    for item in hf_data:
        if item['worker_id']:
            hf_by_worker_id[item['worker_id']].append(item)
    
    print(f"ğŸ“Š í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì—ì„œ {len(hf_by_worker_id)}ê°œì˜ ê³ ìœ í•œ worker_id ë°œê²¬")
    
    # ë§¤ì¹­ëœ ë°ì´í„°ì— í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„° ì—°ê²°
    enhanced_dataset = []
    
    for item in matched_dataset:
        enhanced_item = item.copy()
        
        # worker_idê°€ ìˆëŠ”ì§€ í™•ì¸ (ê¸°ì¡´ ë°ì´í„°ì— worker_idê°€ ìˆì„ ìˆ˜ ìˆìŒ)
        worker_id = item.get('worker_id', '')
        
        if worker_id and worker_id in hf_by_worker_id:
            # í•´ë‹¹ worker_idì˜ í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„° ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒ (ì²« ë²ˆì§¸ ë˜ëŠ” ëœë¤)
            hf_item = hf_by_worker_id[worker_id][0]  # ì²« ë²ˆì§¸ ì„ íƒ
            
            # í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì˜ í•„ë“œë“¤ì„ ì¶”ê°€ (hf_claim, hf_argument ì œì™¸í•˜ê³  hf_ ì ‘ë‘ì‚¬ ì œê±°)
            enhanced_item.update({
                'source': hf_item['source'],
                'rating_initial': hf_item['rating_initial'],
                'rating_final': hf_item['rating_final'],
                'persuasiveness_metric': hf_item['persuasiveness_metric'],
                'prompt_type': hf_item['prompt_type'],
                'original_index': hf_item['original_index']
            })
        
        enhanced_dataset.append(enhanced_item)
    
    return enhanced_dataset

def save_datasets(enhanced_dataset):
    """ê²°ê³¼ ë°ì´í„°ì…‹ì„ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # í†µí•© ë°ì´í„°ì…‹ ì €ì¥
    output_file = f"deception_matched_dataset_o4_mini_{timestamp}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(enhanced_dataset, f, indent=2, ensure_ascii=False)
    
    return output_file

def main():
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    taxonomy_file = 'o4_mini/final_deceptive_taxonomy_20250707_185859.json'
    open_coding_dir = 'o4_mini'
    
    print("ğŸ” ë°ì´í„° ë¡œë”© ì¤‘...")
    
    # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
    taxonomy_mapping = load_taxonomy_data(taxonomy_file)
    open_coding_data = load_open_coding_data(open_coding_dir)
    
    print(f"ğŸ“Š Taxonomyì—ì„œ {len(taxonomy_mapping)}ê°œì˜ sub_strategies ì¶”ì¶œ")
    print(f"ğŸ“Š Open codingì—ì„œ {len(open_coding_data)}ê°œì˜ ê³ ìœ í•œ strategy_name ì¶”ì¶œ")
    
    # í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„°ì…‹ ë¡œë“œ
    hf_data = load_huggingface_dataset()
    if not hf_data:
        print("âŒ í—ˆê¹…í˜ì´ìŠ¤ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        # ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì§„í–‰
        matched_dataset = create_matched_dataset(taxonomy_mapping, open_coding_data)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"deception_matched_dataset_gpt4omini_{timestamp}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(matched_dataset, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“ ê¸°ë³¸ ë§¤ì¹­ ë°ì´í„°ì…‹ ì €ì¥: {output_file}")
        return
    
    # ê¸°ì¡´ ë§¤ì¹­ ìˆ˜í–‰
    print("\nğŸ”— ê¸°ì¡´ ë§¤ì¹­ ìˆ˜í–‰ ì¤‘...")
    matched_dataset = create_matched_dataset(taxonomy_mapping, open_coding_data)
    
    # Worker ID ì—°ê²°
    enhanced_dataset = connect_worker_ids(matched_dataset, hf_data)
    
    # ê²°ê³¼ ì €ì¥
    output_file = save_datasets(enhanced_dataset)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nâœ… ë§¤ì¹­ ì™„ë£Œ!")
    print(f"ğŸ“ í†µí•© ë°ì´í„°ì…‹ ì €ì¥: {output_file}")
    print(f"ğŸ“ ì´ {len(enhanced_dataset)}ê°œ í•­ëª©ì´ í†µí•© ë°ì´í„°ì…‹ì— ì €ì¥ë¨")

if __name__ == "__main__":
    main() 